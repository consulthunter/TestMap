{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:20.953118Z",
     "start_time": "2024-11-27T18:30:20.192770Z"
    }
   },
   "source": [
    "import ast\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, load_dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:20.967666Z",
     "start_time": "2024-11-27T18:30:20.960334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_files_to_dict(directory):\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Walk through directory and subdirectories\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check for CSV files\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                \n",
    "                try:\n",
    "                    # Read the file into a DataFrame\n",
    "                    df = pd.read_csv(file_path, sep=',', quotechar=\"'\", low_memory=False)\n",
    "                    # Store DataFrame in the dictionary with the relative file path as the key\n",
    "                    relative_path = os.path.relpath(file_path, directory)\n",
    "                    data_dict[relative_path] = df\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "directory_path = \"D:/Projects/TestMap/TestMap/Output/\"\n",
    "\n",
    "# Read all files and store in dictionary\n",
    "data_dict = read_files_to_dict(directory_path)"
   ],
   "id": "f6c8253c57dbab2d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:20.976207Z",
     "start_time": "2024-11-27T18:30:20.972771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_csvs(data_dict, keyword):\n",
    "    # Filter DataFrames based on the keyword in the key\n",
    "    filtered_dfs = [df for key, df in data_dict.items() if keyword in key]\n",
    "    \n",
    "    if filtered_dfs:\n",
    "        # Concatenate all filtered DataFrames\n",
    "        combined_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "        # Write combined DataFrame to a CSV file\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(f\"No files found with keyword '{keyword}'\")\n",
    "        \n",
    "# Combine all CSVs with 'test_method' in the key and write to a single CSV\n",
    "test_methods_df = combine_csvs(data_dict, 'test_methods')\n",
    "\n",
    "# Combine all CSVs with 'test_class' in the key and write to a single CSV\n",
    "test_classes_df = combine_csvs(data_dict, 'test_classes')"
   ],
   "id": "81a2b7ffb4d477e9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initial Formatting",
   "id": "6dbc0b4ae47f9e57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test Method Formatting\n",
    "\n",
    "The CSV format from CSharp needs to formatted for Python.\n",
    "\n",
    "Test Method records are different than the Test Class records.\n",
    "\n",
    "They need to formatted differently."
   ],
   "id": "d758f80457573d73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Converting Fields to List of Strings\n",
    "\n",
    "Our list of fields from the testing class is separated with `<<SEP>>`."
   ],
   "id": "d91ed0a698e75799"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.075741Z",
     "start_time": "2024-11-27T18:30:21.072475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_class_fields_to_list(s):\n",
    "    try:\n",
    "        if isinstance(s, str):\n",
    "            s = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "            s = s.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "            temp_list = []\n",
    "            substrs = s.split(\"<<SEP>>\")\n",
    "            \n",
    "            for substr in substrs:\n",
    "                temp_list.append(substr)\n",
    "                \n",
    "            return temp_list\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the function to the column\n",
    "test_methods_df['ClassFields'] = test_methods_df['ClassFields'].apply(convert_class_fields_to_list)"
   ],
   "id": "71e33e320d44d06c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Converting Using Statements to List of Strings",
   "id": "f6b742ed86e3ed56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.090344Z",
     "start_time": "2024-11-27T18:30:21.087335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_usings_to_list(s):\n",
    "    try:\n",
    "        if isinstance(s, str):\n",
    "            s = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "            s = s.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "            temp_list = []\n",
    "            substrs = s.split(\"<<SEP>>\")\n",
    "            \n",
    "            for substr in substrs:\n",
    "                temp_list.append(substr)\n",
    "                \n",
    "            return temp_list\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the function to the column\n",
    "test_methods_df['UsingStatements'] = test_methods_df['UsingStatements'].apply(convert_class_fields_to_list)"
   ],
   "id": "a1636932eb68408f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Converting Method Invocations to a List of Tuples\n",
    "\n",
    "When creating the CSV from the original program, we had a list of tuples in CSharp.\n",
    "\n",
    "CSharp doesn't print lists to strings like Python would. \n",
    "\n",
    "So we had to add keywords and special formatting so we\n",
    "could convert to a list that Python would understand."
   ],
   "id": "4ace4389aec7622a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.104001Z",
     "start_time": "2024-11-27T18:30:21.100658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to add brackets and convert to list of tuples\n",
    "def convert_method_invocations(s):\n",
    "    try:\n",
    "        s = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "        s = s.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "        temp_list = []\n",
    "        substrs = s.split(\"<<SEP>>\")\n",
    "        \n",
    "        for substr in substrs:\n",
    "            substr = substr.lstrip(\"(\").rstrip(\")\")\n",
    "            sub = substr.split('<<TUPLE>>')\n",
    "            if sub[-1] != ' ':\n",
    "                tup = (sub[0].rstrip(', '), sub[-1])\n",
    "                temp_list.append(tup)\n",
    "        return temp_list\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list of tuples: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the function to the column\n",
    "test_methods_df['MethodInvocations'] = test_methods_df['MethodInvocations'].apply(convert_method_invocations)"
   ],
   "id": "1186fc3ed98091c6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Formatting Test Method",
   "id": "f08b20dbe7b54b00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.117223Z",
     "start_time": "2024-11-27T18:30:21.114564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to add brackets and convert to list of tuples\n",
    "def format_test_method(s):\n",
    "    try:\n",
    "        str = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "        str = str.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "        \n",
    "        return str\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list of tuples: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply the function to the column\n",
    "test_methods_df['MethodBody'] = test_methods_df['MethodBody'].apply(convert_method_invocations)"
   ],
   "id": "234d47c4f3e40569",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test Class Formatting",
   "id": "c30b63a46a3a8da4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Converting Fields to List of Strings\n",
    "\n",
    "Our list of fields from the testing class is separated with `<<SEP>>`."
   ],
   "id": "b2a0a98cd3fbcdce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.131607Z",
     "start_time": "2024-11-27T18:30:21.128384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_class_fields_to_list(s):\n",
    "    try:\n",
    "        if isinstance(s, str):\n",
    "            s = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "            s = s.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "            temp_list = []\n",
    "            substrs = s.split(\"<<SEP>>\")\n",
    "            \n",
    "            for substr in substrs:\n",
    "                temp_list.append(substr)\n",
    "                \n",
    "            return temp_list\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the function to the column\n",
    "test_classes_df['ClassFields'] = test_classes_df['ClassFields'].apply(convert_class_fields_to_list)"
   ],
   "id": "c37dc5e3a31509c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Converting Using Statements to List of Strings",
   "id": "3b3e376f1a91f5dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.156203Z",
     "start_time": "2024-11-27T18:30:21.153172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_usings_to_list(s):\n",
    "    try:\n",
    "        if isinstance(s, str):\n",
    "            s = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "            s = s.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "            temp_list = []\n",
    "            substrs = s.split(\"<<SEP>>\")\n",
    "            \n",
    "            for substr in substrs:\n",
    "                temp_list.append(substr)\n",
    "                \n",
    "            return temp_list\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the function to the column\n",
    "test_classes_df['UsingStatements'] = test_classes_df['UsingStatements'].apply(convert_class_fields_to_list)"
   ],
   "id": "b0a1a4cb7ab18bb1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Format Code",
   "id": "2b5d56e83e7a22e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.162668Z",
     "start_time": "2024-11-27T18:30:21.159563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to add brackets and convert to list of tuples\n",
    "def format_code(s):\n",
    "    s = str(s)\n",
    "    try:\n",
    "        st = s.replace(\"<<NEWLINE>>\", \"\\n\")\n",
    "        st = st.replace(\"<<SINGLE-QUOTE>>\", \"\\'\")\n",
    "\n",
    "        return st\n",
    "        \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting string to list of tuples: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply the function to the column\n",
    "test_classes_df['ClassBody'] = test_classes_df['ClassBody'].apply(format_code)\n",
    "test_classes_df['SourceBody'] = test_classes_df['SourceBody'].astype(str).apply(format_code)"
   ],
   "id": "4fda507a94742209",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove Any Empties",
   "id": "6b38f85012c72959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.176762Z",
     "start_time": "2024-11-27T18:30:21.174064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_methods_df_filtered = test_methods_df[test_methods_df['MethodInvocations'].apply(lambda x: len(x) > 0)]\n",
    "test_classes_df_filtered = test_classes_df[test_classes_df['SourceBody'].apply(lambda x: len(x) > 0)]"
   ],
   "id": "58d2895099d2d347",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.209320Z",
     "start_time": "2024-11-27T18:30:21.206778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop duplicates\n",
    "df_methods_dropped = test_methods_df_filtered.drop_duplicates(subset=['MethodBody'])\n",
    "df_classes_dropped = test_classes_df_filtered.drop_duplicates(subset=['ClassBody'])"
   ],
   "id": "da35bc81b01dea26",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.236806Z",
     "start_time": "2024-11-27T18:30:21.233889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find duplicates based on specific columns\n",
    "test_methods_dup = df_methods_dropped.duplicated(subset=['MethodBody'])\n",
    "test_classes_dup = df_classes_dropped.duplicated(subset=['ClassBody'])\n",
    "\n",
    "# Count the number of duplicate rows based on specified columns\n",
    "num_method_duplicates_based_on_columns = test_methods_dup.sum()\n",
    "num_class_duplicates_based_on_columns = test_classes_dup.sum()\n",
    "\n",
    "\n",
    "print(f\"Number of duplicate rows based on specific columns: {num_method_duplicates_based_on_columns}\")\n",
    "print(f\"Number of duplicate rows based on specific columns: {num_class_duplicates_based_on_columns}\")"
   ],
   "id": "f9fa93ba4e1162f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on specific columns: 0\n",
      "Number of duplicate rows based on specific columns: 0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.250021Z",
     "start_time": "2024-11-27T18:30:21.242270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df_methods_dropped.to_csv(os.path.join(\"data\", \"test_methods_full.csv\"), index=False)\n",
    "df_classes_dropped.to_csv(os.path.join(\"data\", \"test_classes_full.csv\"), index=False)\n",
    "\n",
    "df_test_method_full = pd.read_csv(os.path.join(\"data\", \"test_methods_full.csv\"))\n",
    "df_test_classes_full = pd.read_csv(os.path.join(\"data\", \"test_classes_full.csv\"))"
   ],
   "id": "795634192a51d115",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:21.284325Z",
     "start_time": "2024-11-27T18:30:21.273895Z"
    }
   },
   "cell_type": "code",
   "source": "original_method_dataset = Dataset.from_pandas(df_test_method_full)",
   "id": "1cbc0ce546c4ee9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:22.047239Z",
     "start_time": "2024-11-27T18:30:21.307385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upload to huggingface\n",
    "# upload originals\n",
    "test_method_original_name = \"Distaste1194/CSharpTestMethods\"\n",
    "original_method_dataset.push_to_hub(test_method_original_name, private=True)"
   ],
   "id": "937c282ed1566c3e",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6747653d-10373feb1ee5b0a76741f33e;f85c0f65-79bb-49ce-b2e4-417b7b7f1057)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 406\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Upload to huggingface\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# upload originals\u001B[39;00m\n\u001B[0;32m      3\u001B[0m test_method_original_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistaste1194/csharp_test_methods_original\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m \u001B[43moriginal_method_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpush_to_hub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_method_original_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprivate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:5426\u001B[0m, in \u001B[0;36mDataset.push_to_hub\u001B[1;34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001B[0m\n\u001B[0;32m   5422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSplit name should match \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_split_re\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   5424\u001B[0m api \u001B[38;5;241m=\u001B[39m HfApi(endpoint\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mHF_ENDPOINT, token\u001B[38;5;241m=\u001B[39mtoken)\n\u001B[1;32m-> 5426\u001B[0m repo_url \u001B[38;5;241m=\u001B[39m \u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_repo\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5427\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5428\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5429\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5430\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprivate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprivate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   5432\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5433\u001B[0m repo_id \u001B[38;5;241m=\u001B[39m repo_url\u001B[38;5;241m.\u001B[39mrepo_id\n\u001B[0;32m   5435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m revision \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m revision\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrefs/pr/\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m   5436\u001B[0m     \u001B[38;5;66;03m# We do not call create_branch for a PR reference: 400 Bad Request\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3531\u001B[0m, in \u001B[0;36mHfApi.create_repo\u001B[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001B[0m\n\u001B[0;32m   3528\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   3530\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3531\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3532\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m   3533\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exist_ok \u001B[38;5;129;01mand\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m409\u001B[39m:\n\u001B[0;32m   3534\u001B[0m         \u001B[38;5;66;03m# Repo already exists and `exist_ok=True`\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Projects\\TestMap\\Notebooks\\preprocess\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[1;32m--> 477\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6747653d-10373feb1ee5b0a76741f33e;f85c0f65-79bb-49ce-b2e4-417b7b7f1057)\n\nInvalid username or password."
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Class Level",
   "id": "3d4ed69aa49d84db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:22.051214400Z",
     "start_time": "2024-10-18T01:02:09.837886Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 92,
   "source": [
    "\n",
    "original_class_dataset = Dataset.from_pandas(df_test_classes_full)"
   ],
   "id": "33c2236a5d37b717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:22.051214400Z",
     "start_time": "2024-10-18T01:02:13.045947Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "234209e4d5504a48a0ddcbc5dfef64e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3659d218ab1b4e54be4c983db31ec36c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8990b23cc4c744a88ce5447f94639d35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ae999ea5a354977971348a857e4a1d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d1535bab67343c2939d4ce90e9dfbfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47d0a348cf08458196a12ccafefaed0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Distaste1194/csharp_test_classes_formatted_validation/commit/618acbda044b6bf1619d210a739774417f8ad707', commit_message='Upload dataset', commit_description='', oid='618acbda044b6bf1619d210a739774417f8ad707', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Distaste1194/csharp_test_classes_formatted_validation', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Distaste1194/csharp_test_classes_formatted_validation'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93,
   "source": [
    "# Upload to huggingface\n",
    "# upload originals\n",
    "test_class_original_name = \"Distaste1194/CSharpTestClasses\"\n",
    "original_class_dataset.push_to_hub(test_class_original_name, private=True)"
   ],
   "id": "458f258507915a74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
